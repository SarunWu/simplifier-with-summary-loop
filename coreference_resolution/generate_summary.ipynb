{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# main/extract_text_method5_trainset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils.eval_rouge import cal_rouge\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kneskung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') # one time execution\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "import networkx as nx\n",
    "from heapq import nlargest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pickle, time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "ts = time.time()\n",
    "import sentence_transformers as sbert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "sbmodel = sbert.SentenceTransformer(model_name_or_path='all-MiniLM-L6-v2', device='cuda')\n",
    "ts = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.24.1'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import spacy #2.3.8 with numpy==1.21\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.10.1'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import allennlp\n",
    "allennlp.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/634M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79ce17049db848dda88165ac884bdf1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from generate_summary.intersection_strategy import IntersectionStrategy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "coref = IntersectionStrategy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['tagger', 'parser', 'ner']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ncontractions = [\"n\\'t\", \"\\'d\", \"\\'ll\", \"\\'m\", \"\\'re\", \"\\'s\", \"\\'ve\"]\\nSTOP_WORDS.update(contractions)'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS = set(\n",
    "    \"\"\"\n",
    "a about above across after afterwards again against all almost alone along\n",
    "already also although always am among amongst amount an and another any anyhow\n",
    "anyone anything anyway anywhere are around as at\n",
    "back be became because become becomes becoming been before beforehand behind\n",
    "being below beside besides between beyond both bottom but by\n",
    "call can cannot ca could\n",
    "did do does doing done down due during\n",
    "each eight either eleven else elsewhere empty enough even ever every\n",
    "everyone everything everywhere except\n",
    "few fifteen fifty first five for former formerly forty four from front full\n",
    "further\n",
    "get give go\n",
    "had has have he hence her here hereafter hereby herein hereupon hers herself\n",
    "him himself his how however hundred\n",
    "i if in indeed into is it its itself\n",
    "keep\n",
    "last latter latterly least less\n",
    "just\n",
    "made make many may me meanwhile might mine more moreover most mostly move much\n",
    "must my myself\n",
    "name namely neither never nevertheless next nine no nobody none noone nor not\n",
    "nothing now nowhere\n",
    "of off often on once one only onto or other others otherwise our ours ourselves\n",
    "out over own\n",
    "part per perhaps please put\n",
    "quite\n",
    "rather re really regarding\n",
    "same say see seem seemed seeming seems serious several she should show side\n",
    "since six sixty so some somehow someone something sometime sometimes somewhere\n",
    "still such\n",
    "take ten than that the their them themselves then thence there thereafter\n",
    "thereby therefore therein thereupon these they third this those though three\n",
    "through throughout thru thus to together too top toward towards twelve twenty\n",
    "two\n",
    "under until up unless upon us used using\n",
    "various very very via was we well were what whatever when whence whenever where\n",
    "whereafter whereas whereby wherein whereupon wherever whether which while\n",
    "whither who whoever whole whom whose why will with within without would\n",
    "yet you your yours yourself yourselves\n",
    "cnn ll ve lrb rrb -PRON-\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "contractions = [\"n't\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\"]\n",
    "STOP_WORDS.update(contractions)\n",
    "\n",
    "for apostrophe in [\"‘\", \"’\"]:\n",
    "    for stopword in contractions:\n",
    "        STOP_WORDS.add(stopword.replace(\"'\", apostrophe))\n",
    "\"\"\"\n",
    "contractions = [\"n't\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\"]\n",
    "STOP_WORDS.update(contractions)\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "remove_list = [\".\", \",\", \"'\", \"\\\"\", '``', \"'\", ',', '-', '`', \"''\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "news_list = pd.read_pickle(\"data/sample_news.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def find_tfidf(content):\n",
    "    vectorizer = TfidfVectorizer(stop_words=STOP_WORDS, ngram_range=(1, 1))\n",
    "    x = vectorizer.fit_transform(content)\n",
    "    vectorizer.get_feature_names_out()\n",
    "    df = pd.DataFrame(x.toarray(),  columns=vectorizer.get_feature_names_out())\n",
    "    df = df.stack().reset_index()\n",
    "    df = df.rename(columns={0:'tfidf', 'level_0': 'document', 'level_1': 'term'})\n",
    "    df2 = df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)\n",
    "    tf_term = df2['term'].tolist()\n",
    "    return tf_term"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def find_tfidf_idx(idx):\n",
    "    content = [news_list[idx]['content']]\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 1))\n",
    "    x = vectorizer.fit_transform(content)\n",
    "    vectorizer.get_feature_names_out()\n",
    "    df = pd.DataFrame(x.toarray(),  columns=vectorizer.get_feature_names_out())\n",
    "    df = df.stack().reset_index()\n",
    "    df = df.rename(columns={0:'tfidf', 'level_0': 'document', 'level_1': 'term'})\n",
    "    df2 = df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)\n",
    "    tf_term = df2['term'].tolist()\n",
    "    return tf_term"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def find_tfidf2term(content):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(2, 2))\n",
    "    x = vectorizer.fit_transform(content)\n",
    "    vectorizer.get_feature_names_out()\n",
    "    df = pd.DataFrame(x.toarray(),  columns=vectorizer.get_feature_names_out())\n",
    "    df = df.stack().reset_index()\n",
    "    df = df.rename(columns={0:'tfidf', 'level_0': 'document', 'level_1': 'term'})\n",
    "    df2 = df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(20)\n",
    "    tf_term = df2['term'].tolist()\n",
    "    return tf_term"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def showtext(idx=0):\n",
    "    text = [news_list[idx]['content']]\n",
    "    t = find_tfidf(text)\n",
    "    print(\"TF-IDF\")\n",
    "    print(t)\n",
    "    print(\"\\nSummary\")\n",
    "    print(news_list[idx]['summary'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def showtext2term(idx=0):\n",
    "    text = [news_list[idx]['content']]\n",
    "    t = find_tfidf2term(text)\n",
    "    print(\"TF-IDF\")\n",
    "    print(t)\n",
    "    print(\"\\nSummary\")\n",
    "    print(news_list[idx]['summary'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def showtext_ent(idx):\n",
    "    summary = news_list[idx]['summary']\n",
    "    doc = nlp(summary)\n",
    "    displacy.serve(doc, style=\"ent\")\n",
    "    print(\"=======================================\")\n",
    "    content = news_list[idx]['content']\n",
    "    doc = nlp(content)\n",
    "    displacy.serve(doc, style=\"ent\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def filter_and_sort_score(score_dict, num_sent=11):\n",
    "    res = nlargest(num_sent, score_dict, key = score_dict.get)\n",
    "    res.sort()\n",
    "    #print(res)\n",
    "    # return sorted list\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def make_sentence(sent_list, sorted_score_dict):\n",
    "    \"\"\"\n",
    "    sent_list : list of tokenized sentence of each article\n",
    "    sorted_score_dict that we want to make sentence\n",
    "    \"\"\"\n",
    "    #len_sent_list = len(sent_list)\n",
    "    #print(len_sent_list)\n",
    "    final_sent_list = []\n",
    "    for i in sorted_score_dict:\n",
    "        #print(i)\n",
    "        if i <= len(sent_list)-1:\n",
    "            final_sent_list.append(sent_list[i])\n",
    "\n",
    "    # join into string\n",
    "    #result = ' '.join(final_sent_list).replace(',', ' ')\n",
    "    result = ' '.join(final_sent_list)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def make_sentence_coref(sent_list, sorted_score_dict, final_sent_list):\n",
    "    \"\"\"\n",
    "    sent_list : list of tokenized sentence of each article\n",
    "    sorted_score_dict that we want to make sentence\n",
    "    \"\"\"\n",
    "    #len_sent_list = len(sent_list)\n",
    "    #print(len_sent_list)\n",
    "    for i in sorted_score_dict:\n",
    "        #print(i)\n",
    "        #if i <= len_sent_list-1:\n",
    "        final_sent_list.append(sent_list[i])\n",
    "\n",
    "    # join into string\n",
    "    #result = ' '.join(final_sent_list).replace(',', ' ')\n",
    "    result = ' '.join(final_sent_list)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Marseille , France -LRB- CNN -RRB- The French prosecutor leading an investigation into the crash of Germanwings Flight 9525 insisted Wednesday that he was not aware of any video footage from on board the plane . Marseille prosecutor Brice Robin told CNN that `` so far no videos were used in the crash investigation . '' He added , `` A person who has such a video needs to immediately give it to the investigators . '' Robin 's comments follow claims by two magazines , German daily Bild and French Paris Match , of a cell phone video showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French Alps . All 150 on board were killed . Paris Match and Bild reported that the video was recovered from a phone at the wreckage site . The two publications described the supposed video , but did not post it on their websites . The publications said that they watched the video , which was found by a source close to the investigation . `` One can hear cries of ` My God ' in several languages , '' Paris Match reported . `` Metallic banging can also be heard more than three times , perhaps of the pilot trying to open the cockpit door with a heavy object . Towards the end , after a heavy shake , stronger than the others , the screaming intensifies . Then nothing . '' `` It is a very disturbing scene , '' said Julian Reichelt , editor-in-chief of Bild online . An official with France 's accident investigation agency , the BEA , said the agency is not aware of any such video . Lt. Col. Jean-Marc Menichini , a French Gendarmerie spokesman in charge of communications on rescue efforts around the Germanwings crash site , told CNN that the reports were `` completely wrong '' and `` unwarranted . '' Cell phones have been collected at the site , he said , but that they `` had n't been exploited yet . '' Menichini said he believed the cell phones would need to be sent to the Criminal Research Institute in Rosny sous-Bois , near Paris , in order to be analyzed by specialized technicians working hand-in-hand with investigators . But none of the cell phones found so far have been sent to the institute , Menichini said . Asked whether staff involved in the search could have leaked a memory card to the media , Menichini answered with a categorical `` no . '' Reichelt told `` Erin Burnett : Outfront '' that he had watched the video and stood by the report , saying Bild and Paris Match are `` very confident '' that the clip is real . He noted that investigators only revealed they 'd recovered cell phones from the crash site after Bild and Paris Match published their reports . `` That is something we did not know before . ... Overall we can say many things of the investigation were n't revealed by the investigation at the beginning , '' he said . What was mental state of Germanwings co-pilot ? German airline Lufthansa confirmed Tuesday that co-pilot Andreas Lubitz had battled depression years before he took the controls of Germanwings Flight 9525 , which he 's accused of deliberately crashing last week in the French Alps . Lubitz told his Lufthansa flight training school in 2009 that he had a `` previous episode of severe depression , '' the airline said Tuesday . Email correspondence between Lubitz and the school discovered in an internal investigation , Lufthansa said , included medical documents he submitted in connection with resuming his flight training . The announcement indicates that Lufthansa , the parent company of Germanwings , knew of Lubitz 's battle with depression , allowed him to continue training and ultimately put him in the cockpit . Lufthansa , whose CEO Carsten Spohr previously said Lubitz was 100 % fit to fly , described its statement Tuesday as a `` swift and seamless clarification '' and said it was sharing the information and documents -- including training and medical records -- with public prosecutors . Spohr traveled to the crash site Wednesday , where recovery teams have been working for the past week to recover human remains and plane debris scattered across a steep mountainside . He saw the crisis center set up in Seyne-les-Alpes , laid a wreath in the village of Le Vernet , closer to the crash site , where grieving families have left flowers at a simple stone memorial . Menichini told CNN late Tuesday that no visible human remains were left at the site but recovery teams would keep searching . French President Francois Hollande , speaking Tuesday , said that it should be possible to identify all the victims using DNA analysis by the end of the week , sooner than authorities had previously suggested . In the meantime , the recovery of the victims ' personal belongings will start Wednesday , Menichini said . Among those personal belongings could be more cell phones belonging to the 144 passengers and six crew on board . Check out the latest from our correspondents . The details about Lubitz 's correspondence with the flight school during his training were among several developments as investigators continued to delve into what caused the crash and Lubitz 's possible motive for downing the jet . A Lufthansa spokesperson told CNN on Tuesday that Lubitz had a valid medical certificate , had passed all his examinations and `` held all the licenses required . '' Earlier , a spokesman for the prosecutor 's office in Dusseldorf , Christoph Kumpa , said medical records reveal Lubitz suffered from suicidal tendencies at some point before his aviation career and underwent psychotherapy before he got his pilot 's license . Kumpa emphasized there 's no evidence suggesting Lubitz was suicidal or acting aggressively before the crash . Investigators are looking into whether Lubitz feared his medical condition would cause him to lose his pilot 's license , a European government official briefed on the investigation told CNN on Tuesday . While flying was `` a big part of his life , '' the source said , it 's only one theory being considered . Another source , a law enforcement official briefed on the investigation , also told CNN that authorities believe the primary motive for Lubitz to bring down the plane was that he feared he would not be allowed to fly because of his medical problems . Lubitz 's girlfriend told investigators he had seen an eye doctor and a neuropsychologist , both of whom deemed him unfit to work recently and concluded he had psychological issues , the European government official said . But no matter what details emerge about his previous mental health struggles , there 's more to the story , said Brian Russell , a forensic psychologist . `` Psychology can explain why somebody would turn rage inward on themselves about the fact that maybe they were n't going to keep doing their job and they 're upset about that and so they 're suicidal , '' he said . `` But there is no mental illness that explains why somebody then feels entitled to also take that rage and turn it outward on 149 other people who had nothing to do with the person 's problems . '' Germanwings crash compensation : What we know . Who was the captain of Germanwings Flight 9525 ? CNN 's Margot Haddad reported from Marseille and Pamela Brown from Dusseldorf , while Laura Smith-Spark wrote from London . CNN 's Frederik Pleitgen , Pamela Boykoff , Antonia Mortensen , Sandrine Amiel and Anna-Maja Rappard contributed to this report .\""
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = news_list[0]['content']\n",
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "\"-LRB- CNN -RRB- A Duke student has admitted to hanging a noose made of rope from a tree near a student union , university officials said Thursday . The prestigious private school did n't identify the student , citing federal privacy laws . In a news release , it said the student was no longer on campus and will face student conduct review . The student was identified during an investigation by campus police and the office of student affairs and admitted to placing the noose on the tree early Wednesday , the university said . Officials are still trying to determine if other people were involved . Criminal investigations into the incident are ongoing as well . Students and faculty members marched Wednesday afternoon chanting `` We are not afraid . We stand together , '' after pictures of the noose were passed around on social media . At a forum held on the steps of Duke Chapel , close to where the noose was discovered at 2 a.m. , hundreds of people gathered . `` You came here for the reason that you want to say with me , ` This is no Duke we will accept . This is no Duke we want . This is not the Duke we 're here to experience . And this is not the Duke we 're here to create , ' '' Duke President Richard Brodhead told the crowd . The incident is one of several recent racist events to affect college students . Last month a fraternity at the University of Oklahoma had its charter removed after a video surfaced showing members using the N-word and referring to lynching in a chant . Two students were expelled . In February , a noose was hung around the neck of a statue of a famous civil rights figure at the University of Mississippi . A statement issued by Duke said there was a previous report of hate speech directed at students on campus . In the news release , the vice president for student affairs called the noose incident a `` cowardly act . '' `` To whomever committed this hateful and stupid act , I just want to say that if your intent was to create fear , it will have the opposite effect , '' Larry Moneta said Wednesday . Duke University is a private college with about 15,000 students in Durham , North Carolina . CNN 's Dave Alsup contributed to this report .\""
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = news_list[5]['content']\n",
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "tf = find_tfidf_idx(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n"
     ]
    }
   ],
   "source": [
    "ner_lst = nlp.pipe_labels['ner']\n",
    "print(len(ner_lst))\n",
    "print(ner_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "NER_list = ['DATE', 'EVENT', 'FAC', 'GPE', 'LOC', 'MONEY', 'NORP', 'ORG', 'PERSON',  'TIME',]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN ORG\n",
      "CNN ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    if ent.text.lower() in tf and ent.label_ in NER_list:\n",
    "        print(ent.text,  ent.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "['said',\n 'lubitz',\n 'cnn',\n 'crash',\n 'investigation',\n 'told',\n 'germanwings',\n 'video',\n 'flight',\n 'site']"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_tfidf_idx(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kneskung/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages/spacy/displacy/__init__.py:94: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <title>displaCy</title>\n    </head>\n\n    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n<figure style=\"margin-bottom: 6rem\">\n<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Former \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    GOP\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n representative compares President \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Obama\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n to \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Andreas Lubitz\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n . . \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Bachmann\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n said with possible \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Iran\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n deal , \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Obama\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n will fly `` entire nation into the rocks '' . Reaction on social media ? She was blasted by \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Facebook\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n commenters .</div>\n</figure>\n</body>\n</html></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 48] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mshowtext_ent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m11\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[21], line 4\u001B[0m, in \u001B[0;36mshowtext_ent\u001B[0;34m(idx)\u001B[0m\n\u001B[1;32m      2\u001B[0m summary \u001B[38;5;241m=\u001B[39m news_list[idx][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummary\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      3\u001B[0m doc \u001B[38;5;241m=\u001B[39m nlp(summary)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mdisplacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstyle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43ment\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=======================================\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m content \u001B[38;5;241m=\u001B[39m news_list[idx][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages/spacy/displacy/__init__.py:97\u001B[0m, in \u001B[0;36mserve\u001B[0;34m(docs, style, page, minify, options, manual, port, host)\u001B[0m\n\u001B[1;32m     94\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(Warnings\u001B[38;5;241m.\u001B[39mW011)\n\u001B[1;32m     96\u001B[0m render(docs, style\u001B[38;5;241m=\u001B[39mstyle, page\u001B[38;5;241m=\u001B[39mpage, minify\u001B[38;5;241m=\u001B[39mminify, options\u001B[38;5;241m=\u001B[39moptions, manual\u001B[38;5;241m=\u001B[39mmanual)\n\u001B[0;32m---> 97\u001B[0m httpd \u001B[38;5;241m=\u001B[39m \u001B[43msimple_server\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mUsing the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m visualizer\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(style))\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mServing on http://\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m ...\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(host, port))\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.10/wsgiref/simple_server.py:154\u001B[0m, in \u001B[0;36mmake_server\u001B[0;34m(host, port, app, server_class, handler_class)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_server\u001B[39m(\n\u001B[1;32m    151\u001B[0m     host, port, app, server_class\u001B[38;5;241m=\u001B[39mWSGIServer, handler_class\u001B[38;5;241m=\u001B[39mWSGIRequestHandler\n\u001B[1;32m    152\u001B[0m ):\n\u001B[1;32m    153\u001B[0m     \u001B[38;5;124;03m\"\"\"Create a new WSGI server listening on `host` and `port` for `app`\"\"\"\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m     server \u001B[38;5;241m=\u001B[39m \u001B[43mserver_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandler_class\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m     server\u001B[38;5;241m.\u001B[39mset_app(app)\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m server\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.10/socketserver.py:452\u001B[0m, in \u001B[0;36mTCPServer.__init__\u001B[0;34m(self, server_address, RequestHandlerClass, bind_and_activate)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bind_and_activate:\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 452\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_activate()\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.10/wsgiref/simple_server.py:50\u001B[0m, in \u001B[0;36mWSGIServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mserver_bind\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     \u001B[43mHTTPServer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_environ()\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.10/http/server.py:136\u001B[0m, in \u001B[0;36mHTTPServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mserver_bind\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     \u001B[43msocketserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTCPServer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     host, port \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_address[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_name \u001B[38;5;241m=\u001B[39m socket\u001B[38;5;241m.\u001B[39mgetfqdn(host)\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.10/socketserver.py:466\u001B[0m, in \u001B[0;36mTCPServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mallow_reuse_address:\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39msetsockopt(socket\u001B[38;5;241m.\u001B[39mSOL_SOCKET, socket\u001B[38;5;241m.\u001B[39mSO_REUSEADDR, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 466\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_address\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_address \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mgetsockname()\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 48] Address already in use"
     ]
    }
   ],
   "source": [
    "showtext_ent(11)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "\"-LRB- CNN -RRB- Michele Bachmann is comparing President Obama to the co-pilot of the doomed Germanwings flight . `` With his Iran deal , Barack Obama is for the 300 million souls of the United States what Andreas Lubitz was for the 150 souls on the German Wings flight - a deranged pilot flying his entire nation into the rocks , '' the Minnesota Republican and former representative wrote in a Facebook comment posted March 31 . `` After the fact , among the smoldering remains of American cities , the shocked survivors will ask , why did he do it ? '' Andreas Lubitz , the co-pilot of Germanwings Flight 9525 , is accused by authorities of deliberately crashing the plane in the French Alps . He died in the crash along with 149 other crew and passengers . The motive of the March 24 crash is under investigation , though investigators are looking in to whether Lubitz feared a medical condition would cause him to lose his pilot 's license . Many comments posted on her Facebook page blasted the former representative . Melissa Coca wrote , `` Comparing this tragedy to anything is moronic and despicable . '' Michael J Pristash wrote , `` Your allusion is so inappropriate and divisive , not to mention disrespectful on so many levels . Shame on you . '' Some also accused her of taking desperate measures to stay in the public eye . Lynda Anderson wrote , `` Posting outrageous things in a pathetic attempt to stay relevant ? '' Negotiations are coming down to the wire between Iran , the United States and other nations on restricting Tehran 's nuclear program to prevent the ability to develop an atomic bomb . One deadline passed Tuesday , but there is a June 30 deadline for a comprehensive deal -- with all technical and diplomatic impasses fully worked out . Bachmann is no stranger to voicing her opinion on the President 's dealing with Iran , personally telling him to `` bomb Iran '' during the 2014 White House Christmas Party . `` I turned to the president and I said , something to the effect of , ` Mr. President , you need to bomb the Iranian nuclear facilities , because if you do n't , Iran will have a nuclear weapon on your watch and the course of world history will change , ' '' she told the Washington Free Beacon . The congresswoman , who sought the GOP presidential nomination in 2012 , said Obama had a `` condescending smile on his face and laughed at me . '' She said he told her : `` Well Michele , it 's just not that easy . ''\""
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_list[11]['content']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "idx = 37"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "text = news_list[idx]['content']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "sent_list = sent_tokenize(text)\n",
    "len_sent_list = len(sent_list)\n",
    "first3_sent = sent_list[:3]\n",
    "last_sent = sent_list[-1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "select_sent = sent_list[3:len_sent_list-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "select_text = ' '.join(select_sent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kneskung/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n"
     ]
    }
   ],
   "source": [
    "select_text_coref = coref.resolve_coreferences(select_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "\"`` Operating accelerators for the benefit of the physics community is what CERN 's here for , '' CERN Director-General Rolf Heuer said on the organization 's website . `` Today , CERN 's heart beats once more to the rhythm of the LHC . '' the LHC generates up to 600 million particles per second , with a beam circulating for 10 hours , traveling more than 6 billion miles -LRB- more than 10 billion kilometers -RRB- -- the distance from Earth to Neptune and back again . At near light-speed , a proton in the LHC makes 11,245 circuits per second . It took thousands of scientists , engineers and technicians decades to devise and build the particle accelerator , housed in a tunnel between Lake Geneva and the Jura mountain range . The purpose of the lengthy project is to recreate the conditions that existed moments after the `` Big Bang '' -- the scientific theory said to explain the creation of the universe . By replicating the energy density and temperature , scientists hope to uncover how the universe evolved . Our current , limited , knowledge is based on what 's called The Standard Model of particle physics . `` But we know that The Standard Model of particle physics is not complete , '' Dr. Mike Lamont , operations group leader at the LHC , told CNN in March . The burning questions that remain include the origin of mass and why some particles are very heavy , while others have no mass at all ; a unified description of all the fundamental forces such as gravity ; and uncovering dark matter and dark energy , since visible matter accounts for only 4 percent of the universe . the LHC could also question the idea that the universe is only made of matter , despite the theory that antimatter must have been produced in the same amounts at the time of the Big Bang . CERN says the energies achievable by the LHC have only ever been found in nature . The machine alone costs approximately three billion euros -LRB- about $ 3.3 billion -RRB- , paid for by member countries of CERN and contributions by non-member nations . CERN also asserts that CERN's guidelines for the protection of the environment and personnel comply with standards set by Swiss and French laws and a European Council Directive . Scientists and physics enthusiasts will be waiting with bated breath as the LHC ventures into the great unknown . `` After two years of effort , the LHC is in great shape , '' said CERN Director for Accelerators and Technology , Frédérick Bordry . `` But the most important step is still to come when CERN increase the energy of the beams to new record levels . ''\""
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_text_coref"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "Python 3.10 (pytorch)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}