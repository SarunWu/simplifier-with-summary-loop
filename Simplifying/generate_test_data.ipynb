{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import simplifier\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_test = 100\n",
    "name_to_save = \"../coreference_resolution/data/\" + str(num_test) + \"_news.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model and tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_name = \"google/pegasus-cnn_dailymail\"\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(\"cpu\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "print(\"model and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load original dataset (pickle)\n",
    "with open(\"../coreference_resolution/data/sample_news.pkl\", \"rb\") as f:\n",
    "    news_list = pickle.load(f)\n",
    "\n",
    "news_list = news_list[:num_test]\n",
    "len(news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of articles to process is  100 \n",
      "\n",
      "DONE! generate and simplify\n"
     ]
    }
   ],
   "source": [
    "# generate and simplify\n",
    "\n",
    "print(\"number of articles to process is \", num_test, '\\n')\n",
    "\n",
    "total_news_list = []\n",
    "\n",
    "ts = time.time()\n",
    "for idx, news in tqdm(enumerate(news_list)):\n",
    "\n",
    "    # stop at chosen num_test\n",
    "    if idx == num_test:\n",
    "        break\n",
    "\n",
    "    # progress printing\n",
    "    print(\"Start \", idx)\n",
    "    if (idx != 0) and (idx % 100 == 0):\n",
    "        print(\"Writing story {} of {}; {:.2f} percent done. Time spent: {:.2f}\".format(\n",
    "            idx, num_test, float(idx) * 100.0 / float(num_test), time.time() - ts))\n",
    "\n",
    "\n",
    "    news_dict = dict()\n",
    "    content = news['content']\n",
    "    ref_summary = news['summary']\n",
    "\n",
    "    # generate summary for original input (post-process)\n",
    "\n",
    "    inputs = tokenizer(content, max_length=1024, return_tensors=\"pt\", truncation=True).to(\"cpu\")\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, max_length=50)\n",
    "    gen_summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    # simplify the summary (post-process)\n",
    "    gen_sim_summary = simplifier.simplify(gen_summary)\n",
    "\n",
    "    # simplify the input (pre-process)\n",
    "    sim_content = simplifier.simplify(content)\n",
    "\n",
    "    # generate summary for simplified input (pre-process)\n",
    "\n",
    "    inputs = tokenizer(sim_content, max_length=1024, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, max_length=50)\n",
    "    sim_gen_summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[\n",
    "        0]\n",
    "\n",
    "    # combine all into dict\n",
    "    print(\"Start 5\", idx)\n",
    "    news_dict[\"ori_content\"] = content\n",
    "    news_dict[\"ref_summary\"] = ref_summary\n",
    "    news_dict[\"gen_summary\"] = gen_summary\n",
    "    news_dict[\"gen_sim_summary\"] = gen_sim_summary\n",
    "    news_dict[\"sim_content\"] = sim_content\n",
    "    news_dict[\"sim_gen_summary\"] = sim_gen_summary\n",
    "\n",
    "    # append to a global list\n",
    "    total_news_list.append(news_dict)\n",
    "\n",
    "total_news_list.to_pickle(name_to_save)\n",
    "print(\"DONE! generate and simplify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "Python 3.10 (pytorch)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fa7967d3cc3d8110a8b6cb0884c4a46939acf6c239a16e9f5aa6820a7a812c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}